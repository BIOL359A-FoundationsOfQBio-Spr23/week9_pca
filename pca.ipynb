{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A  | Principal Component Analysis\n",
    "### Spring 2023, Week 9\n",
    "<hr>\n",
    "Learning Objectives:\n",
    "\n",
    "  - Understand motivations of dimension reduction\n",
    "  - Discuss applications towards data visualization and data exploration\n",
    "  - Interpret the results of a principal component analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr23/week9_pca\n",
    "!mkdir ./data\n",
    "!cp week9_pca/data/* ./data\n",
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (8,8)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':FIG_SIZE}) \n",
    "sns.set_style()\n",
    "plt.rc(\"axes.spines\", top=False, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorizer(x, y):\n",
    "    \"\"\"\n",
    "    Map x-y coordinates to a rgb color\n",
    "    \"\"\"\n",
    "    r = min(1, 1+y/3)\n",
    "    b = min(1, 1-y/3)\n",
    "    g = 1/4 + x/16\n",
    "    return (r, g, b)\n",
    "\n",
    "def gen_basic_plot(A, eigen=False):\n",
    "    xvals = np.linspace(-4, 4, 9)\n",
    "    yvals = np.linspace(-3, 3, 7)\n",
    "    xygrid = np.column_stack([[x, y] for x in xvals for y in yvals])\n",
    "\n",
    "    uvgrid = np.dot(A, xygrid)\n",
    "    # Map grid coordinates to colors\n",
    "    colors = list(map(colorizer, xygrid[0], xygrid[1]))\n",
    "\n",
    "    # Plot grid points \n",
    "    plt.scatter(xygrid[0], xygrid[1], s=40, c=colors, edgecolor=\"none\")\n",
    "    # Set axis limits\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"Original Grid\")\n",
    "    if eigen:\n",
    "        eigen_values, eigen_vectors = np.linalg.eig(A)\n",
    "    \n",
    "        eig_vec1 = eigen_vectors[:,0]\n",
    "        eig_vec2 = eigen_vectors[:,1]\n",
    "        np.set_printoptions(precision=3)\n",
    "        print(f\"Eigen Vector: {eig_vec1} - Eigen Value: {eigen_values[0]:.2f}\")\n",
    "        print(f\"Eigen Vector: {eig_vec2} - Eigen Value: {eigen_values[1]:.2f}\")\n",
    "        origin = [0,0]\n",
    "        plt.quiver(*origin, *eig_vec1, color=['r'], scale=21)\n",
    "        plt.quiver(*origin, *eig_vec2, color=['b'], scale=21) \n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    plt.scatter(uvgrid[0], uvgrid[1], s=40, c=colors, edgecolor=\"none\")\n",
    "    # Set axis limits\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Transformed Grid\")\n",
    "    if eigen:\n",
    "        plt.quiver(*origin, *eig_vec1, color=['r'], scale=21)\n",
    "        plt.quiver(*origin, *eig_vec2, color=['b'], scale=21)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_wrapper(a=2,b=1,c=-1,d=1, eigen=False):\n",
    "    A = np.column_stack([[a, b], [c, d]])\n",
    "    gen_basic_plot(A, eigen=eigen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and PCA Example (Questions 1-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with the following matrix:\n",
    "\n",
    "$$\\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\\\ \\end{bmatrix}$$\n",
    "\n",
    "(This matrix has no real eigen vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrapper(a=2,b=1,c=-1,d=1, eigen=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix has real eigen vectors:\n",
    "\n",
    "$$\\begin{bmatrix} 5 & 1 \\\\ 4 & 2 \\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrapper(a=5, b=1, c=4,d=2, eigen=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing the Covariance Matrix:\n",
    "\n",
    "$$ var(x) = E[(x-\\mu_{x})^2] $$\n",
    "Recall when we defined the covariance:\n",
    "\n",
    "$$ cov(x,y) = E[x-\\mu_{x}]E[y-\\mu_{y}] $$\n",
    "\n",
    "So for example, If I had a data matrix, with N observations, and 3 features - a, b, and c - I will define my covariance matrix $\\Sigma$ as:\n",
    " \n",
    "$$ \\Sigma = \\begin{bmatrix} var(a) & cov(a,b) & cov(a,c)\\\\ cov(b,a) & var(b)& cov(b,c) \\\\ cov(c,a) & cov(c,b) & var(c) \\end{bmatrix}$$\n",
    "\n",
    "Which we usually estimate with:\n",
    "\n",
    "$$ \\frac{1}{N-1} XX^T = \\hat{\\Sigma} $$\n",
    "\n",
    "(Look familiar? This is the outer product with a normalization factor!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_pca():\n",
    "    # Sample data\n",
    "    np.random.seed(42)\n",
    "    x = np.random.normal(0, 1, 100)\n",
    "    y = 2 * x + np.random.normal(0, 1, 100)\n",
    "    data = np.vstack((x, y)).T\n",
    "\n",
    "    # Centering the data\n",
    "    centered_data = data - np.mean(data, axis=0)\n",
    "\n",
    "    # Performing PCA\n",
    "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "    eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Sorting eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eig_values)[::-1]\n",
    "    eig_values = eig_values[sorted_indices]\n",
    "    eig_vectors = eig_vectors[:, sorted_indices]\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.scatter(centered_data[:, 0], centered_data[:, 1], alpha=0.5, label='Data')\n",
    "\n",
    "    # Plotting the best PCA direction\n",
    "    best_direction = eig_vectors[:, 0]\n",
    "    projected_data = np.dot(centered_data, best_direction)\n",
    "    max_value = np.max(projected_data)\n",
    "    min_value = np.min(projected_data)\n",
    "    t = np.linspace(min_value, max_value, 100)\n",
    "    plt.plot(t * best_direction[0], t * best_direction[1], color='red', label='Best PCA')\n",
    "\n",
    "    # Plotting a suboptimal direction\n",
    "    suboptimal_direction = eig_vectors[:, 1]\n",
    "    projected_data = np.dot(centered_data, suboptimal_direction)\n",
    "    max_value = np.max(projected_data)\n",
    "    min_value = np.min(projected_data)\n",
    "    t = np.linspace(min_value, max_value, 100)\n",
    "    plt.plot(t * suboptimal_direction[0], t * suboptimal_direction[1], color='green', label='Suboptimal PCA')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('PCA Demonstration')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# Calling the function\n",
    "demonstrate_pca()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a toy dataset\n",
    "\n",
    "Notice the direction where the most variance occurs. This line will be slightly different from our linear regression line, because in PCA we're treating these variables the same. (We care about the error in both the x and y direction, rather than simply the y-direction).\n",
    "\n",
    "We then generate the eigen vector of our covariance matrix, and plot it over the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_pca():\n",
    "    rng = np.random.RandomState(1)\n",
    "    X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "    plt.scatter(X[:, 0], X[:, 1])\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    \n",
    "    # plot data\n",
    "    plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "    for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "        v = vector * 3 * np.sqrt(length)\n",
    "        draw_vector(pca.mean_, pca.mean_ + v)\n",
    "    plt.axis('equal'); \n",
    "    plt.show()\n",
    "    \n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    color='k',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "example_pca()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate System\n",
    "Now that we have the eigenvectors, we can do two things with them.\n",
    "First, we're going to look at using them as a new coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinatesystem_example():\n",
    "    rng = np.random.RandomState(1)\n",
    "    X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "    pca = PCA(n_components=2, whiten=True)\n",
    "    pca.fit(X)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.15)\n",
    "\n",
    "    # plot data\n",
    "    ax[0].scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "    for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "        v = vector * 3 * np.sqrt(length)\n",
    "        draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "    ax[0].axis('equal');\n",
    "    ax[0].set(xlabel='x', ylabel='y', title='input')\n",
    "\n",
    "    # plot principal components\n",
    "    X_pca = pca.transform(X)\n",
    "    ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "    draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "    draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "    ax[1].axis('equal')\n",
    "    ax[1].set(xlabel='component 1', ylabel='component 2',\n",
    "              title='principal components',\n",
    "              xlim=(-5, 5), ylim=(-3, 3.1))\n",
    "coordinatesystem_example()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "The other option we have is to simply use one principal component, and represent our data with one vector rather than two. What does this accomplish? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_example():\n",
    "    rng = np.random.RandomState(1)\n",
    "    X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "    pca = PCA(n_components=2, whiten=True)\n",
    "    pca.fit(X)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.15)\n",
    "\n",
    "    # plot data\n",
    "    ax[0].scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "    ax[0].axis('equal');\n",
    "    ax[0].set(xlabel='x', ylabel='y', title='input')\n",
    "\n",
    "    # plot principal components\n",
    "    pca = PCA(n_components=1, whiten=True)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    X_new = pca.inverse_transform(X_pca)\n",
    "    ax[1].scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "    ax[1].scatter(X_new[:, 0], X_new[:, 1], color=\"blue\", alpha=0.8)\n",
    "    ax[1].axis('equal')\n",
    "    ax[1].set(xlabel='x', ylabel='y',\n",
    "              title='projected data (First PC)')\n",
    "    \n",
    "projection_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerformPCA(X, n_dimensions=10, plot=True):\n",
    "    \"\"\"\n",
    "    Uses sklearn PCA tool to perform PCA\n",
    "    input:\n",
    "    X: Pandas Dataframe or Numpy Array of features\n",
    "    n_dimensions: Number of PCs to fit\n",
    "    \n",
    "    output:\n",
    "    X_pca: Pandas dataframe with column titles of PC1,...,PCn\n",
    "    \"\"\"\n",
    "    X_standardized = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=n_dimensions)\n",
    "    pca.fit(X_standardized)\n",
    "    X_pca_array = pca.transform(X_standardized)\n",
    "    column_names = ['PC{}'.format(i+1) for i in range(n_dimensions)] \n",
    "    X_pca = pd.DataFrame(X_pca_array, columns=column_names)\n",
    "    if plot:\n",
    "        plt.plot(column_names, np.cumsum(pca.explained_variance_ratio_), 'o--')\n",
    "        plt.title('Skree Plot', fontsize=TITLE_FONT)\n",
    "        plt.xlabel('Number of PCs', fontsize=LABEL_FONT)\n",
    "        plt.ylabel('Total Percent Variance Explained', fontsize=LABEL_FONT)\n",
    "        plt.ylim(0,1)\n",
    "        plt.show()\n",
    "    return X_pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are going to explore a couple datasets that we've looked at before with PCA (Question 4 + 5)\n",
    "The first dataset we'll look at is penguin characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins=load_penguins()\n",
    "penguins.dropna(inplace=True)\n",
    "features=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\",\"body_mass_g\"]\n",
    "label='species'\n",
    "sns.pairplot(penguins, vars=features, corner=True, hue=\"species\", markers=[\"o\", \"s\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X = penguins[features]\n",
    "penguins_y = penguins[label]\n",
    "penguins_PCA_df = PerformPCA(penguins_X, n_dimensions=4)\n",
    "penguins_PCA_df = penguins_PCA_df.join(penguins_y)\n",
    "sns.scatterplot(x='PC1',y='PC2',hue='species',data=penguins_PCA_df,style='species',markers=[\"o\", \"s\", \"D\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the penguin plot of PC1 vs. PC2, we can see that we can visualize and seperate the penguins reasonably well even though we went from 4 features (bill length, bill depth, flipper length, body mass) down to 2 features on the chart (PC1, PC2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"species\", y=\"PC1\", hue=\"species\",\n",
    "                 data=penguins_PCA_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"species\", y=\"PC2\", hue=\"species\",\n",
    "                 data=penguins_PCA_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are going to look at our trusty breast cancer dataset (Question 6 + 7)\n",
    "\n",
    "Remember, we could build fairly effective classification models with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cancer_raw = load_breast_cancer()\n",
    "print(cancer_raw.DESCR)\n",
    "tumor_features = pd.DataFrame(cancer_raw.data, columns=cancer_raw.feature_names)\n",
    "tumor = pd.DataFrame(cancer_raw.target, columns=['tumor'])\n",
    "tumor_nominal = tumor.replace({'tumor': {0: 'benign', 1: 'malignant'}})\n",
    "cancer_df = pd.concat([tumor_features, tumor_nominal], axis=1)\n",
    "tumor_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df = PerformPCA(tumor_features)\n",
    "PCA_df = PCA_df.join(tumor_nominal)\n",
    "sns.scatterplot(x='PC1',y='PC2',style='tumor',hue='tumor',data=PCA_df,markers=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if I plot different PCs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='PC4',y='PC5',style='tumor',hue='tumor',data=PCA_df,markers=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
